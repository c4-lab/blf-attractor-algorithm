{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53fa55b-8c61-46a9-85e5-e438cd1841d4",
   "metadata": {},
   "source": [
    "# Idenitfying Attractors and Dynamics on Belief Landscapes\n",
    "\n",
    "This notebook will help you identify and label attractors for your belief landscape.  This should be an interactive process.  At a minimum, you will need a dataframe with 5 columns:  `u_id, x, y, dt (datetime)`\n",
    "\n",
    "Set the path to the datafile in the following cell (presumed to be a feather file) and a name for the resulting output file.\n",
    "\n",
    "See here: https://chatgpt.com/share/66e35048-6bd4-8001-86c6-48aa854e48b8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3a204c4-99ef-45bf-9de8-a56b3f50c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path_to_data = \"./local_data/data_for_icwsm/icwsmtuned_random_invert_0_umap_clustered_35_trace.feather\"\n",
    "output_name = \"./local_data/labeled_trace_climate_change_icwsm23.feather\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b88123-fd81-4633-a389-e00d28d550a9",
   "metadata": {},
   "source": [
    "## First, calculate a grid to establish a vector field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce3bc71-3381-415c-a3f0-46f13e6e8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "\n",
    "trace = pd.read_feather(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ce64e-15a0-4931-b880-0fdd91a025b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trace[[\"u_id\",\"x\",\"y\",\"dt\"]]\n",
    "grid_size = 50\n",
    "df['x_grid'] = ((df['x'] - df['x'].min()) / (df['x'].max() - df['x'].min()) * (grid_size - 1)).astype(int)\n",
    "df['y_grid'] = ((df['y'] - df['y'].min()) / (df['y'].max() - df['y'].min()) * (grid_size - 1)).astype(int)\n",
    "\n",
    "# Sort by user and datetime to ensure correct transition order\n",
    "df.sort_values(['u_id', 'dt'], inplace=True)\n",
    "\n",
    "# Calculate transitions\n",
    "df['from_x'] = df['x_grid'].shift(1)\n",
    "df['from_y'] = df['y_grid'].shift(1)\n",
    "df['to_x'] = df['x_grid']\n",
    "df['to_y'] = df['y_grid']\n",
    "\n",
    "# Filter out rows that do not represent a transition (first occurrence for each user)\n",
    "transitions = df[df['u_id'] == df['u_id'].shift(1)]\n",
    "\n",
    "transitions['dx'] = transitions['to_x'] - transitions['from_x']\n",
    "transitions['dy'] = transitions['to_y'] - transitions['from_y']\n",
    "tx = transitions[['from_x','from_y','dx','dy']]\n",
    "tx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5be499-223e-4144-8a28-1ce114c160ae",
   "metadata": {},
   "source": [
    "## Smoothed aggregation\n",
    "\n",
    "The strategy here is to: \n",
    "\n",
    "1. use a simple Bayesian update with an uninformed prior to estimate the vector and variance of each cell and\n",
    "2. use a smoothing procedure (weighted local averages) to remove spurious data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8658727c-5ea7-4129-86ce-8772a333cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Utiliity functions to convert between dataframes and grids\n",
    "\n",
    "def dataframe_to_grid(df):\n",
    "    # Create mappings from from_x and from_y to indices\n",
    "    unique_x = np.sort(df['from_x'].unique())\n",
    "    unique_y = np.sort(df['from_y'].unique())\n",
    "    x_to_index = {x: idx for idx, x in enumerate(unique_x)}\n",
    "    y_to_index = {y: idx for idx, y in enumerate(unique_y)}\n",
    "\n",
    "    # Initialize grids for mu and variance\n",
    "    mu_dx_grid = np.full((len(unique_x), len(unique_y)), np.nan)\n",
    "    mu_dy_grid = np.full((len(unique_x), len(unique_y)), np.nan)\n",
    "    var_dx_grid = np.full((len(unique_x), len(unique_y)), np.nan)\n",
    "    var_dy_grid = np.full((len(unique_x), len(unique_y)), np.nan)\n",
    "\n",
    "    # Fill grids with values from DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        i = x_to_index[row['from_x']]\n",
    "        j = y_to_index[row['from_y']]\n",
    "        mu_dx_grid[i, j] = row['mu_dx']\n",
    "        mu_dy_grid[i, j] = row['mu_dy']\n",
    "        var_dx_grid[i, j] = row['var_dx']\n",
    "        var_dy_grid[i, j] = row['var_dy']\n",
    "    return mu_dx_grid, mu_dy_grid, var_dx_grid, var_dy_grid\n",
    "\n",
    "def grid_to_dataframe(mu_dx_grid, mu_dy_grid, x_grid=None, y_grid = None, var_dx_grid = None, var_dy_grid = None):\n",
    "    data = []\n",
    "    for i in range(mu_dx_grid.shape[0]):\n",
    "        for j in range(mu_dx_grid.shape[1]):\n",
    "            \n",
    "            x_coord = i if x_grid is None else x_grid[i,j]\n",
    "            y_coord = j if y_grid is None else y_grid[i,j]\n",
    "            row = {\n",
    "                'from_x': x_coord,\n",
    "                'from_y': y_coord,\n",
    "                'mu_dx': mu_dx_grid[i, j],\n",
    "                'mu_dy': mu_dy_grid[i, j]}\n",
    "            if var_dx_grid is not None and var_dy_grid is not None:\n",
    "                row['var_dx']=var_dx_grid[i, j]\n",
    "                row['var_dy'] = var_dy_grid[i, j]\n",
    "            data.append(row)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Bayesian updating\n",
    "\n",
    "def bayesian_update(mu_prior, var_prior, mean_sample, var_sample, count, epsilon=1e-10):\n",
    "    if count == 0:\n",
    "        return mu_prior, var_prior\n",
    "    if count==1:\n",
    "        var_sample = (var_prior + epsilon) / 2\n",
    "    if var_sample==0:\n",
    "        var_sample = epsilon\n",
    "    precision_prior = 1 / var_prior\n",
    "    precision_sample = 1 / var_sample\n",
    "    combined_precision = precision_prior + count * precision_sample\n",
    "    mu_posterior = (precision_prior * mu_prior + count * precision_sample * mean_sample) / combined_precision\n",
    "    var_posterior = count / combined_precision\n",
    "    return mu_posterior, var_posterior\n",
    "\n",
    "\n",
    "def prepare_cell_data(df):\n",
    "    # Group by 'from_x' and 'from_y' and calculate mean and variance\n",
    "    grouped = df.groupby(['from_x', 'from_y'])\n",
    "    summary = grouped.agg({\n",
    "        'dx': ['mean', 'var'],\n",
    "        'dy': ['mean', 'var','count'],\n",
    "        \n",
    "    }).reset_index()\n",
    "    summary.columns = ['from_x', 'from_y', 'dx_mean', 'dx_var', 'dy_mean', 'dy_var','n']\n",
    "    # Replace NaN variances (which occur if there's only one sample) with a small positive number\n",
    "    summary.fillna({'dx_var': 1e-10, 'dy_var': 1e-10}, inplace=True)\n",
    "    return summary\n",
    "\n",
    "def process_df(df, mu_prior, var_prior):\n",
    "    data = prepare_cell_data(df)\n",
    "    results = []\n",
    "    for _, row in data.iterrows():\n",
    "        # Bayesian updates for dx and dy\n",
    "        mu_dx, var_dx = bayesian_update(mu_prior, var_prior, row['dx_mean'], row['dx_var'], row['n'])\n",
    "        mu_dy, var_dy = bayesian_update(mu_prior, var_prior, row['dy_mean'], row['dy_var'], row['n'])\n",
    "        results.append([row['from_x'], row['from_y'], mu_dx, var_dx, mu_dy, var_dy])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['from_x', 'from_y', 'mu_dx', 'var_dx', 'mu_dy', 'var_dy'])\n",
    "\n",
    "    # Spatial smoothing would be applied here, using spatial_weighted_average\n",
    "    return results_df\n",
    "\n",
    "# Spatial smoothing\n",
    "\n",
    "def spatial_weighted_average(grid, i, j, radius, mu_grid, var_grid):\n",
    "    weighted_mu, weighted_var, total_weight = 0, 0, 0\n",
    "    for di in range(-radius, radius + 1):\n",
    "        for dj in range(-radius, radius + 1):\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < grid.shape[0] and 0 <= nj < grid.shape[1] and (di != 0 or dj != 0):\n",
    "                weight = 1 / (1 + abs(di) + abs(dj))  # Example weighting scheme\n",
    "                weighted_mu += weight * mu_grid[ni, nj]\n",
    "                weighted_var += weight * var_grid[ni, nj]\n",
    "                total_weight += weight\n",
    "    if total_weight > 0:\n",
    "        return weighted_mu / total_weight, weighted_var / total_weight\n",
    "    return mu_grid[i, j], var_grid[i, j]\n",
    "\n",
    "def apply_spatial_smoothing(mu_dx_grid, mu_dy_grid, var_dx_grid, var_dy_grid, radius):\n",
    "    smoothed_mu_dx_grid = np.copy(mu_dx_grid)\n",
    "    smoothed_mu_dy_grid = np.copy(mu_dy_grid)\n",
    "    smoothed_var_dx_grid = np.copy(var_dx_grid)\n",
    "    smoothed_var_dy_grid = np.copy(var_dy_grid)\n",
    "\n",
    "    for i in range(mu_dx_grid.shape[0]):\n",
    "        for j in range(mu_dy_grid.shape[1]):\n",
    "            if not np.isnan(mu_dx_grid[i, j]):\n",
    "                smoothed_mu_dx_grid[i, j], smoothed_var_dx_grid[i, j] = spatial_weighted_average(\n",
    "                    mu_dx_grid, i, j, radius, mu_dx_grid, var_dx_grid)\n",
    "                smoothed_mu_dy_grid[i, j], smoothed_var_dy_grid[i, j] = spatial_weighted_average(\n",
    "                    mu_dy_grid, i, j, radius, mu_dy_grid, var_dy_grid)\n",
    "\n",
    "    return smoothed_mu_dx_grid, smoothed_mu_dy_grid, smoothed_var_dx_grid, smoothed_var_dy_grid\n",
    "\n",
    "\n",
    "# Plotting for inspection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_vector_field(df, scale=1):\n",
    "    \n",
    "    # Plotting the vector field\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    Q = ax.quiver(\n",
    "        df['from_x'], df['from_y'], \n",
    "        df['mu_dx']*scale, df['mu_dy']*scale,  # Scale the vectors\n",
    "        angles='xy', scale_units='xy', scale=1,\n",
    "        pivot='mid'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X coordinate')\n",
    "    ax.set_ylabel('Y coordinate')\n",
    "    ax.set_title('Vector Field')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_vector_field_with_var(df, scale=1):\n",
    "    # Calculate the average variance for coloring\n",
    "    df['avg_var'] = (df['var_dx'] + df['var_dy']) / 2\n",
    "    \n",
    "    # Plotting the vector field\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    Q = ax.quiver(\n",
    "        df['from_x'], df['from_y'], \n",
    "        df['mu_dx']*scale, df['mu_dy']*scale,  # Scale the vectors\n",
    "        df['avg_var'],  # Use average variance for coloring\n",
    "        angles='xy', scale_units='xy', scale=1,\n",
    "        cmap='viridis', pivot='mid'\n",
    "    )\n",
    "    \n",
    "    # Adding a color bar to show the variance\n",
    "    cbar = fig.colorbar(Q, ax=ax)\n",
    "    cbar.set_label('Average Variance of Vector Components')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X coordinate')\n",
    "    ax.set_ylabel('Y coordinate')\n",
    "    ax.set_title('Vector Field with Variance Indication')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83301ef2-3b0f-41bc-8b00-64ee77a136c5",
   "metadata": {},
   "source": [
    "The following calls depend on a couple of parameters.\n",
    "\n",
    "- `process_df` - takes the transitions data (`tx`) and then your prior estimate of mean and variance for each vector.  Note that the code doesn't support more sophisticated priors right now, but starting with mu=0 and var=1 is a sensible choice anyway.\n",
    "- `apply_spatial_smoothing` - the last parameter (radius) indicates how large a neighborhood you want to consider when smoothing the data.  1 is a good choice. Larger numbers will result in a more uniform vector field.\n",
    "- `plot_vector_field_with_var` - the scale parameter (the last one) just sets a factor for plotting vector arrows.  This is purely for visualization and does not affect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d223b-145b-48c4-bd37-238f22cac02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a mean = 0 and var = 1\n",
    "# Note that we are converting between table and grid-based representations to facilitate smoothing\n",
    "\n",
    "smoothed = process_df(tx, 0,1)\n",
    "mu_dx,mu_dy,var_dx,var_dy = dataframe_to_grid(smoothed)\n",
    "mu_dx,mu_dy,var_dx,var_dy = apply_spatial_smoothing(mu_dx,mu_dy,var_dx,var_dy,1)\n",
    "averaged = grid_to_dataframe(mu_dx,mu_dy,var_dx_grid=var_dx,var_dy_grid=var_dy)\n",
    "plot_vector_field_with_var(averaged,.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c52612-d320-48e3-b58e-e8858ddd82b1",
   "metadata": {},
   "source": [
    "## Extracting attractors\n",
    "\n",
    "Now that we've prepared things, all we really need to do is run simulations to see where each trajectory winds up; after that we will just cluster endpoints.  There are a couple of details here - first, interpolating will give us some additional granularity, allowing us to resolve smaller attractors.  Also, we're using DBScan to identify attractors - this might not always work right, especially if we have adjacent attractors lying on either side of a separatrix. This is something we'll take care of another time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ceeb24-9538-491e-9931-7d443ce8fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from scipy.spatial import Delaunay\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from scipy.ndimage import generic_filter\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "class DiscreteVectorFieldAnalyzer:\n",
    "    def __init__(self, df, interpolation_factor=1):\n",
    "        self.df = df\n",
    "        self.interpolation_factor = interpolation_factor\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        # Extract original data\n",
    "        x = self.df['from_x'].values\n",
    "        y = self.df['from_y'].values\n",
    "        u = self.df['mu_dx'].values\n",
    "        v = self.df['mu_dy'].values\n",
    "        \n",
    "        # Create fine grid\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        y_min, y_max = y.min(), y.max()\n",
    "        self.x_fine = np.linspace(x_min, x_max, int((x_max - x_min) * self.interpolation_factor))\n",
    "        self.y_fine = np.linspace(y_min, y_max, int((y_max - y_min) * self.interpolation_factor))\n",
    "        self.X_fine, self.Y_fine = np.meshgrid(self.x_fine, self.y_fine)\n",
    "        \n",
    "        # Prepare points for interpolation\n",
    "        points = np.column_stack((x, y))\n",
    "        \n",
    "        # Create Delaunay triangulation for the original points\n",
    "        tri = Delaunay(points)\n",
    "        \n",
    "        # Interpolate U and V using LinearNDInterpolator\n",
    "        self.U_interp = LinearNDInterpolator(points, u, fill_value=np.nan)\n",
    "        self.V_interp = LinearNDInterpolator(points, v, fill_value=np.nan)\n",
    "        \n",
    "        # Apply interpolation\n",
    "        self.U_fine = self.U_interp(self.X_fine, self.Y_fine)\n",
    "        self.V_fine = self.V_interp(self.X_fine, self.Y_fine)\n",
    "        \n",
    "        # Create mask for valid data points\n",
    "        # fine_points = np.column_stack((self.X_fine.ravel(), self.Y_fine.ravel()))\n",
    "        # self.valid_mask = tri.find_simplex(fine_points) >= 0\n",
    "        # self.valid_mask = self.valid_mask.reshape(self.X_fine.shape)\n",
    "\n",
    "        # Create a more restrictive mask for valid data points\n",
    "        fine_points = np.column_stack((self.X_fine.ravel(), self.Y_fine.ravel()))\n",
    "        in_simplex = tri.find_simplex(fine_points) >= 0\n",
    "        has_valid_values = ~np.isnan(self.U_fine.ravel()) & ~np.isnan(self.V_fine.ravel())\n",
    "        self.valid_mask = (in_simplex & has_valid_values).reshape(self.X_fine.shape)\n",
    "        \n",
    "        # Apply mask to U_fine and V_fine\n",
    "        self.U_fine[~self.valid_mask] = np.nan\n",
    "        self.V_fine[~self.valid_mask] = np.nan\n",
    "        \n",
    "        self.grid_shape = self.X_fine.shape\n",
    "\n",
    "\n",
    "    def generate_trajectory(self, start_index, max_steps=1000, step_size=0.1):\n",
    "        trajectory = [start_index]\n",
    "        current_point = np.array([self.X_fine[start_index[0], start_index[1]], \n",
    "                                  self.Y_fine[start_index[0], start_index[1]]])\n",
    "        visited = set([start_index])\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            i, j = self.discretize_point(current_point)\n",
    "            if not self.valid_mask[i, j]:\n",
    "                print(\"We are out of bounds\")\n",
    "                \n",
    "            delta = self.vector_field(i, j)\n",
    "            \n",
    "            if np.all(np.abs(delta) < 1e-6):\n",
    "                break  # Stopping point detected\n",
    "            \n",
    "            # Euler integration step\n",
    "            next_point = current_point + delta * step_size\n",
    "            \n",
    "            # Discretize the new point to get the next grid index\n",
    "            next_index = self.discretize_point(next_point)\n",
    "            \n",
    "            if not (0 <= next_index[0] < self.grid_shape[0] and 0 <= next_index[1] < self.grid_shape[1]):\n",
    "                break  # Out of bounds\n",
    "\n",
    "            if not self.valid_mask[next_index[0], next_index[1]]:\n",
    "                break \n",
    "            \n",
    "            if next_index in visited:\n",
    "                break  # Cycle detected\n",
    "            \n",
    "            visited.add(next_index)\n",
    "            trajectory.append(next_index)\n",
    "            current_point = next_point\n",
    "        \n",
    "        return trajectory\n",
    "\n",
    "\n",
    "    def process_point(self, args):\n",
    "        index, step_size = args\n",
    "        trajectory = self.generate_trajectory(index, step_size=step_size)\n",
    "        return trajectory\n",
    "\n",
    "    def analyze_vector_field(self, step_size=0.1):\n",
    "        valid_indices = list(zip(*np.where(self.valid_mask)))\n",
    "        \n",
    "        # Create a list of tuples containing both the index and step_size\n",
    "        args = [(index, step_size) for index in valid_indices]\n",
    "        \n",
    "        with mp.Pool() as pool:\n",
    "            trajectories = pool.map(self.process_point, args)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        return trajectories\n",
    "\n",
    "\n",
    "    def discretize_point(self, point):\n",
    "        \"\"\"Convert a continuous point to the nearest grid index.\"\"\"\n",
    "        j = np.clip(np.searchsorted(self.x_fine, point[0]), 0, self.grid_shape[1] - 1)\n",
    "        i = np.clip(np.searchsorted(self.y_fine, point[1]), 0, self.grid_shape[0] - 1)\n",
    "        return (int(i), int(j))\n",
    "        \n",
    "   \n",
    "\n",
    "    def vector_field(self, i, j):\n",
    "        \"\"\"Return the vector field at the given grid indices.\"\"\"\n",
    "        return np.array([self.U_fine[i, j], self.V_fine[i, j]])\n",
    "\n",
    " \n",
    "    def find_attractors_and_basins(self, trajectories, min_quality=10, threshold=0.1, samples=5):\n",
    "        valid_indices = list(zip(*np.where(self.valid_mask)))\n",
    "        # Convert indices to coordinates for clustering\n",
    "        end_indices = [t[-1] for t in trajectories]\n",
    "        end_points = np.array([(self.X_fine[i, j], self.Y_fine[i, j]) for i, j in end_indices])\n",
    "        \n",
    "        clustering = DBSCAN(eps=threshold, min_samples=samples).fit(end_points)\n",
    "        \n",
    "        unique_labels = np.unique(clustering.labels_)\n",
    "        print(f\"Unique labels: {unique_labels}\")\n",
    "        attractors = []\n",
    "        valid_labels = {}  # Dictionary to map DBSCAN labels to new attractor indices\n",
    "        new_label = 0  # Counter for new labels\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            if label != -1:  # -1 is noise in DBSCAN\n",
    "                cluster_points = end_points[clustering.labels_ == label]\n",
    "                quality = 0\n",
    "                cluster_indices = np.where(clustering.labels_ == label)[0]\n",
    "                for idx in cluster_indices:\n",
    "                    quality += len(trajectories[idx])\n",
    "                if quality > min_quality:\n",
    "                    attractors.append(np.mean(cluster_points, axis=0))\n",
    "                    valid_labels[label] = new_label\n",
    "                    new_label += 1\n",
    "        \n",
    "        attractors = np.array(attractors)\n",
    "        print(f\"Number of attractors after pruning: {len(attractors)}\")\n",
    "        \n",
    "        # Assign basins\n",
    "        basins = np.full(self.grid_shape, -1, dtype=int)\n",
    "        for (i, j), label in zip(valid_indices, clustering.labels_):\n",
    "            basins[i, j] = valid_labels.get(label, -1)\n",
    "        \n",
    "        # Fill pruned basins with nearest valid basin\n",
    "        valid_points = []\n",
    "        valid_labels_list = []\n",
    "        for i, j in valid_indices:\n",
    "            if basins[i, j] != -1:\n",
    "                valid_points.append((self.X_fine[i, j], self.Y_fine[i, j]))\n",
    "                valid_labels_list.append(basins[i, j])\n",
    "        \n",
    "        if not valid_points:\n",
    "            print(\"Warning: All attractors were pruned.\")\n",
    "            return attractors, basins  # Return if no valid points (all attractors pruned)\n",
    "        \n",
    "        tree = cKDTree(valid_points)\n",
    "        \n",
    "        pruned_points = []\n",
    "        pruned_indices = []\n",
    "        for i, j in valid_indices:\n",
    "            if basins[i, j] == -1:\n",
    "                pruned_points.append((self.X_fine[i, j], self.Y_fine[i, j]))\n",
    "                pruned_indices.append((i, j))\n",
    "        \n",
    "        if pruned_points:\n",
    "            distances, indices = tree.query(pruned_points, k=1)\n",
    "            for (i, j), idx in zip(pruned_indices, indices):\n",
    "                basins[i, j] = valid_labels_list[idx]\n",
    "        \n",
    "        return attractors, basins\n",
    "\n",
    "    def plot_results(self, attractors, basins):\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        masked_basins = np.ma.masked_where(~self.valid_mask, basins)\n",
    "        plt.imshow(masked_basins, extent=[self.x_fine.min(), self.x_fine.max(), \n",
    "                                          self.y_fine.min(), self.y_fine.max()],\n",
    "                   origin='lower', alpha=0.6, cmap='viridis')\n",
    "        plt.streamplot(self.X_fine, self.Y_fine, self.U_fine, self.V_fine, \n",
    "                       density=3, color='gray', arrowsize=0.5)\n",
    "        plt.scatter(attractors[:, 0], attractors[:, 1], c='red', s=100, label='Attractors')\n",
    "        plt.colorbar(label='Basin of Attraction')\n",
    "        plt.title('Vector Field with Basins of Attraction')\n",
    "        plt.xlabel('X coordinate')\n",
    "        plt.ylabel('Y coordinate')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aff287-049c-4b07-b7bd-ddd35c96a389",
   "metadata": {},
   "source": [
    "Again, some parameter explanations here:\n",
    "\n",
    "- `DiscreteVectorFieldAnalyzer` - the `interpolation_factor` increases the granularity of the vector field.  This helps to resolve smaller attractors - it is a form of bootstrapping, in that we are creating data based on what we already have.  I have found a factor of four to be useful, but YMMV.\n",
    "- `DiscreteVectorFieldAnalyzer.analyze_vector_field` - we are running walks over the vector field using Euler's method.  However, because we are on a discrete grid, step sizes that are too small will generate a single attractor for every grid point.  Too large, and we might only find the most dominant attractors. So, this value ultimately has a large impact on how many attractors you find. You'll want to fiddle with it until it seems like you are getting attractors that roughly correspond to what you see in the vector map.  Note that the next step will allow you to screen out smaller, spurious attractors.  These sometimes appear at the edge of the map where data can be a little soft.\n",
    "- `DiscreteVectorFieldAnalyzer.find_attractors_and_basins` - the `min_quality` parameter is a simple heuristic for pruning smaller, spurious attractors. Quality is just the sum of all trajectory lengths leading to an attractor.  So, if you've got some really small attractors with just a few short paths, the `min_quality` parameter will enable you to prune them. The `threshold` and `samples` parameters are just DBScan's `epsilon` and `min_samples` parameters.  This controls the clustering procedure used to identify attractors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca6c56b-7e04-414a-b530-d885a23f7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = DiscreteVectorFieldAnalyzer(averaged, interpolation_factor=4)\n",
    "trajectories = analyzer.analyze_vector_field(step_size=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0795a6d-c050-4c91-b9d6-cc637a8ed7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attractors, basins = analyzer.find_attractors_and_basins(trajectories,min_quality=40,threshold=1,samples=5)\n",
    "analyzer.plot_results(attractors,basins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695057d-e1dc-49e9-9c0d-08e979e9ed8a",
   "metadata": {},
   "source": [
    "## Finally, smoothing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f439e0ce-4493-46c2-bc21-7abc926f6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import splprep, splev\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "class BasinSmoother:\n",
    "    def __init__(self, basins):\n",
    "        self.basins = basins\n",
    "        #self.attractors = np.array(attractors)\n",
    "        self.num_attractors = len(attractors)\n",
    "        self.shape = basins.shape\n",
    "\n",
    "    def create_basin_mask(self, attractor_index):\n",
    "        return (self.basins == attractor_index).astype(np.uint8)\n",
    "\n",
    "    def smooth_basin(self, mask, smoothing_factor=2, simplification_factor=0.01):\n",
    "        # Ensure odd-sized kernel by adding 1 if smoothing_factor is even\n",
    "        kernel_size = smoothing_factor + 1 if smoothing_factor % 2 == 0 else smoothing_factor\n",
    "        kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "\n",
    "        # Apply morphological closing to fill small holes and smooth edges\n",
    "        closed = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # Find contours of the closed mask\n",
    "        contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if not contours:\n",
    "            return mask\n",
    "        \n",
    "        # Simplify the largest contour\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        epsilon = simplification_factor * cv2.arcLength(largest_contour, True)\n",
    "        approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
    "        \n",
    "        # Create a new mask from the simplified contour\n",
    "        smooth_mask = np.zeros_like(mask)\n",
    "        cv2.drawContours(smooth_mask, [approx], 0, 1, -1)\n",
    "        \n",
    "        # Ensure all original points are included (prevents orphaned points)\n",
    "        smooth_mask = np.logical_or(smooth_mask, mask).astype(np.uint8)\n",
    "        \n",
    "        return smooth_mask\n",
    "\n",
    "    def fill_holes(self, mask):\n",
    "        # Find contours of the mask\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # If no contours found, return the original mask\n",
    "        if not contours:\n",
    "            return mask\n",
    "        \n",
    "        # Create a new mask and fill in the contours\n",
    "        filled_mask = np.zeros_like(mask)\n",
    "        cv2.drawContours(filled_mask, contours, -1, 1, -1)\n",
    "        \n",
    "        return filled_mask\n",
    "\n",
    "    def concave_hull_smooth(self, mask, alpha=0.1, spline_smoothness=0, max_iterations=1000):\n",
    "        # Get boundary points\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if not contours:\n",
    "            return mask\n",
    "        \n",
    "        boundary_points = contours[0].squeeze()\n",
    "        \n",
    "        # Check if boundary_points is empty or has only one point\n",
    "        if boundary_points.size == 0 or len(boundary_points.shape) == 1:\n",
    "            return mask\n",
    "        \n",
    "        # Ensure boundary_points is 2D\n",
    "        if len(boundary_points.shape) > 2:\n",
    "            boundary_points = boundary_points.reshape(-1, 2)\n",
    "        \n",
    "        # Check if we have enough points for triangulation\n",
    "        if len(boundary_points) < 3:\n",
    "            return mask\n",
    "\n",
    "        try:\n",
    "            # Compute Delaunay triangulation\n",
    "            tri = Delaunay(boundary_points)\n",
    "\n",
    "            # Find edges that satisfy alpha criterion\n",
    "            edges = set()\n",
    "            for simplex in tri.simplices:\n",
    "                for i in range(3):\n",
    "                    j = (i + 1) % 3\n",
    "                    p1, p2 = boundary_points[simplex[i]], boundary_points[simplex[j]]\n",
    "                    if np.linalg.norm(p1 - p2) < 1 / alpha:\n",
    "                        edges.add(tuple(sorted((tuple(p1), tuple(p2)))))\n",
    "\n",
    "            # Create ordered edge list\n",
    "            ordered_edges = list(edges)\n",
    "            if not ordered_edges:\n",
    "                return mask\n",
    "            \n",
    "            hull = [ordered_edges[0][0], ordered_edges[0][1]]\n",
    "            used_edges = set([ordered_edges[0]])\n",
    "\n",
    "            iteration = 0\n",
    "            while len(used_edges) < len(edges) and iteration < max_iterations:\n",
    "                last_point = hull[-1]\n",
    "                found_edge = False\n",
    "                for edge in edges:\n",
    "                    if edge in used_edges:\n",
    "                        continue\n",
    "                    if last_point in edge:\n",
    "                        next_point = edge[0] if edge[1] == last_point else edge[1]\n",
    "                        hull.append(next_point)\n",
    "                        used_edges.add(edge)\n",
    "                        found_edge = True\n",
    "                        break\n",
    "                if not found_edge:\n",
    "                    # If no edge is found, break the loop to avoid infinite iteration\n",
    "                    break\n",
    "                iteration += 1\n",
    "\n",
    "            hull = np.array(hull)\n",
    "\n",
    "            # Fit a spline to the hull points\n",
    "            if len(hull) >= 4:  # Need at least 4 points for a periodic spline\n",
    "                tck, u = splprep(hull.T, u=None, s=spline_smoothness, per=1)\n",
    "                u_new = np.linspace(u.min(), u.max(), 1000)\n",
    "                smooth_hull = np.column_stack(splev(u_new, tck))\n",
    "            else:\n",
    "                smooth_hull = hull\n",
    "\n",
    "            # Create a new mask from the smooth hull\n",
    "            smooth_mask = np.zeros_like(mask)\n",
    "            cv2.fillPoly(smooth_mask, [smooth_hull.astype(np.int32)], 1)\n",
    "\n",
    "            # Fill any holes in the smooth mask\n",
    "            smooth_mask = self.fill_holes(smooth_mask)\n",
    "\n",
    "            # Ensure all original points are included\n",
    "            smooth_mask = np.logical_or(smooth_mask, mask).astype(np.uint8)\n",
    "\n",
    "            return smooth_mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error in concave hull smoothing: {e}\")\n",
    "            return mask\n",
    "\n",
    "    def process_basins(self, smooth=True, smoothing_factor=2, simplification_factor=0.01, alpha=0.1, spline_smoothness=0):\n",
    "        # Calculate basin sizes and strengths\n",
    "        basin_sizes = np.array([np.sum(self.basins == i) for i in range(self.num_attractors)])\n",
    "        strengths = 1 / basin_sizes\n",
    "        strengths = strengths / np.max(strengths)  # Normalize\n",
    "\n",
    "        # Sort attractors by increasing strength (decreasing basin size)\n",
    "        sorted_indices = np.argsort(strengths)\n",
    "\n",
    "        processed_basins = np.full(self.shape, -1, dtype=int)\n",
    "\n",
    "        for i in sorted_indices:\n",
    "            mask = self.create_basin_mask(i)\n",
    "            \n",
    "            if smooth:\n",
    "                # Apply initial smoothing\n",
    "                smooth_mask = self.smooth_basin(mask, smoothing_factor, simplification_factor)\n",
    "                # Apply final concave hull smoothing\n",
    "                smooth_mask = self.concave_hull_smooth(smooth_mask, alpha, spline_smoothness)\n",
    "            else:\n",
    "                smooth_mask = mask\n",
    "\n",
    "            # Update the processed basins\n",
    "            processed_basins[smooth_mask == 1] = i\n",
    "\n",
    "        return processed_basins\n",
    "\n",
    "    def visualize_processed_basins(self, processed_basinss):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        # Plot original basins\n",
    "        ax1.imshow(self.basins, cmap='viridis', interpolation='nearest', origin='lower')\n",
    "        \n",
    "        ax1.set_title(\"Original Basins\")\n",
    "        \n",
    "        # Plot processed basins\n",
    "        ax2.imshow(processed_basins, cmap='viridis', interpolation='nearest', origin='lower')\n",
    "       \n",
    "        title = \"Smoothed Basins\"\n",
    "        ax2.set_title(title)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a8830-4832-4b4a-a115-8a1ed8fe4b6a",
   "metadata": {},
   "source": [
    "The attractor basins developed in the previous method usually have fuzzy boundaries, and can be discontinuous.  The `BasinSmoother` is designed to fix this.  It requires tuning and visual investigation.  You are trying to get relatively smooth boundaries with no orphan points (disconnected from their basin) and no \"holes\" in basins.  It is probably easier to just play with things and develop a visual intuition rather than try to understand the parameters, but here's a brief explanation:\n",
    "\n",
    "- `smoothing_factor` -  The first step in the smoother tries to fill in holes in the attractor basins by passing a kernel over the basin and filling in any empty bits.  We are using OpenCV's `morphologyEx` function with a `CLOSE` operation; see [this explanation](https://docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html). The `smoothing_factor` controls the size of the kernel.\n",
    "- `simplification_factor` - After that, we try to find a polygon that approximates the contour of the shape.  Note that if we still have a disconnected basin here or holes our contours will pull those out, so the previous step is pretty important. We are using OpenCV's [`approxPolyDP` function](https://docs.opencv.org/4.x/d3/dc0/group__imgproc__shape.html#ga0012a5fdaea70b8a9970165d98722b4c), and the `simplification_factor` controls `epsilon` in that function.  Larger values will lead to less complex, more approximate polygons.\n",
    "- `alpha` - After the preceding, we then move on to a second pass of smoothing.  We may still at this point have disconnected portions a jagged boundary, so our goal is to fix these problems and generate a smoother boundary.  First we create a triangular mesh using the [`Delaunay` algorithm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Delaunay.html) and then attempt to shink the mesh by deleting edges.  `alpha` controls which edges we keep, with smaller values allowing for a more concave shape (think of this like \"shrink wrapping\" the polygon, where `alpha` is the thickness of the shrink wrap.  Thicker shrink wraps do not resolve as much detail in the underlying object.\n",
    "- `spline_smoothness` - Finally, we draw a spline around the outside for a final pass of smoothing - the `spline_smoothness` parameter controls how smooth the line is, with higher values yeilding smoother lines.\n",
    "\n",
    "You might wonder if all of this is necessary?  Maybe not! But it is quite flexible, and enables us to get a pretty nice boundary.  Future iterations might simplify this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d8412-8c03-4b78-913d-87f2f024411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "smoother = BasinSmoother(basins)\n",
    "\n",
    "# Process with smoothing\n",
    "smoothed_basins = smoother.process_basins(smooth=True, smoothing_factor=7, simplification_factor=0.001, alpha=0.05, spline_smoothness=3)\n",
    "smoother.visualize_processed_basins(smoothed_basins, smooth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c43b58-414a-439a-9876-b569b665fd8d",
   "metadata": {},
   "source": [
    "## Mapping back to trace data\n",
    "\n",
    "The rest of this is just to map the results back to the trace data.  Note that we do a little estimation here, because the raw trace data may have points slightly beyond the original vector field.  We use a nearest neighbors search to assign these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ec54628-740c-49e3-93c7-2fdc0dd11acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def map_basins_to_trace(trace_df, smoothed_basins):\n",
    "    # Get the dimensions of the smoothed basins array\n",
    "    basin_height, basin_width = smoothed_basins.shape\n",
    "    \n",
    "    # Get the min and max of x and y from the original trace data\n",
    "    x_min, x_max = trace_df['x'].min(), trace_df['x'].max()\n",
    "    y_min, y_max = trace_df['y'].min(), trace_df['y'].max()\n",
    "    \n",
    "    def get_basin_index(x, y):\n",
    "        # Normalize the x and y coordinates to [0, 1]\n",
    "        x_norm = (x - x_min) / (x_max - x_min)\n",
    "        y_norm = (y - y_min) / (y_max - y_min)\n",
    "        \n",
    "        # Map to basin indices\n",
    "        basin_x = int(x_norm * (basin_width - 1))\n",
    "        basin_y = int(y_norm * (basin_height - 1))\n",
    "        \n",
    "        # Ensure we don't go out of bounds\n",
    "        basin_x = max(0, min(basin_x, basin_width - 1))\n",
    "        basin_y = max(0, min(basin_y, basin_height - 1))\n",
    "        \n",
    "        return smoothed_basins[basin_y, basin_x]\n",
    "    \n",
    "    # Apply the mapping function to each row in the trace dataframe\n",
    "    trace_df['basin'] = trace_df.apply(lambda row: get_basin_index(row['x'], row['y']), axis=1)\n",
    "    \n",
    "    # Identify unassigned points (assuming -1 is used for unassigned)\n",
    "    unassigned_mask = trace_df['basin'] == -1\n",
    "    \n",
    "    if unassigned_mask.sum() > 0:\n",
    "        # Create a KD-tree of assigned points\n",
    "        assigned_points = trace_df.loc[~unassigned_mask, ['x', 'y']].values\n",
    "        assigned_tree = cKDTree(assigned_points)\n",
    "        \n",
    "        # Find nearest assigned neighbor for each unassigned point\n",
    "        unassigned_points = trace_df.loc[unassigned_mask, ['x', 'y']].values\n",
    "        _, indices = assigned_tree.query(unassigned_points)\n",
    "        \n",
    "        # Assign the basin of the nearest neighbor\n",
    "        trace_df.loc[unassigned_mask, 'basin'] = trace_df.loc[~unassigned_mask, 'basin'].iloc[indices].values\n",
    "    \n",
    "    return trace_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f12a6160-6f93-475e-9c4d-3cad28be5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_trace = map_basins_to_trace(trace,basins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5b6ee94-2fb3-49de-a8e5-5a868f67bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_trace_data_by_basin(trace_df, output_file=None):\n",
    "    # Set the seaborn style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Create the figure and axis objects\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    # Create a color palette for the basins\n",
    "    n_basins = trace_df['basin'].nunique()\n",
    "    palette = sns.color_palette(\"husl\", n_colors=n_basins)\n",
    "\n",
    "    # Create the scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        trace_df['x'], \n",
    "        trace_df['y'],\n",
    "        c=trace_df['basin'],\n",
    "        cmap=plt.cm.colors.ListedColormap(palette),\n",
    "        s=1,  # Small point size\n",
    "        alpha=0.1,  # Low alpha for better visibility of dense areas\n",
    "    )\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_title('Trace Data Colored by Basin', fontsize=16)\n",
    "    ax.set_xlabel('X', fontsize=12)\n",
    "    ax.set_ylabel('Y', fontsize=12)\n",
    "\n",
    "    # Add a color bar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Basin', fontsize=12)\n",
    "    \n",
    "    # Adjust the color bar ticks to show only integer values\n",
    "    cbar.set_ticks(np.arange(n_basins) + 0.5)\n",
    "    cbar.set_ticklabels(range(n_basins))\n",
    "\n",
    "    # Improve the layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if an output file is specified\n",
    "    if output_file:\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved as {output_file}\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379db5a-8774-41d6-8104-deadba4fa820",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trace_data_by_basin(mapped_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e21e7-3733-4d7e-a0e9-52a5bd3e737f",
   "metadata": {},
   "source": [
    "## Process variance\n",
    "\n",
    "We'll also take our original variance estimates from the attractor field and attach those to the data frame.  Once again, there's a little estimation here at the boundaries of the vector field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32efb584-8d26-4198-93c2-25b59f486a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def process_variance(trace_df, x_variance_grid, y_variance_grid):\n",
    "    \"\"\"  \n",
    "    Attaches variance to the original trace data.  Note that this destructively modifies the data frame.\n",
    "    \"\"\"\n",
    "    # 1. Average the variance grids\n",
    "    avg_variance_grid = (x_variance_grid + y_variance_grid) / 2\n",
    "\n",
    "    # 2. Prepare data for interpolation, excluding NaN values\n",
    "    x, y = np.mgrid[0:avg_variance_grid.shape[1], 0:avg_variance_grid.shape[0]]\n",
    "    valid_mask = ~np.isnan(avg_variance_grid)\n",
    "    points = np.column_stack((x[valid_mask], y[valid_mask]))\n",
    "    values = avg_variance_grid[valid_mask]\n",
    "\n",
    "    # Create interpolator\n",
    "    interp_func = LinearNDInterpolator(points, values, fill_value=np.nan)\n",
    "\n",
    "    # 3. Map variance to trace data\n",
    "    x_min, x_max = trace_df['x'].min(), trace_df['x'].max()\n",
    "    y_min, y_max = trace_df['y'].min(), trace_df['y'].max()\n",
    "    x_norm = (trace_df['x'] - x_min) / (x_max - x_min)\n",
    "    y_norm = (trace_df['y'] - y_min) / (y_max - y_min)\n",
    "    scaled_x = x_norm * (avg_variance_grid.shape[1] - 1)\n",
    "    scaled_y = y_norm * (avg_variance_grid.shape[0] - 1)\n",
    "    trace_df['variance'] = interp_func(np.column_stack((scaled_x, scaled_y)))\n",
    "\n",
    "    # 4. Fill NaN values using nearest neighbor\n",
    "    nan_mask = np.isnan(trace_df['variance'])\n",
    "    valid_mask = ~nan_mask\n",
    "    \n",
    "    if nan_mask.sum() > 0:\n",
    "        valid_points = np.column_stack((scaled_x[valid_mask], scaled_y[valid_mask]))\n",
    "        valid_values = trace_df.loc[valid_mask, 'variance'].values\n",
    "        \n",
    "        kdtree = cKDTree(valid_points)\n",
    "        nan_points = np.column_stack((scaled_x[nan_mask], scaled_y[nan_mask]))\n",
    "        \n",
    "        _, indices = kdtree.query(nan_points, k=1)\n",
    "        trace_df.loc[nan_mask, 'variance'] = valid_values[indices]\n",
    "    \n",
    "    return trace_df\n",
    "\n",
    "\n",
    "# Usage\n",
    "# trace_df = ... # Your trace dataframe\n",
    "# x_variance_grid = ... # Your x variance grid\n",
    "# y_variance_grid = ... # Your y variance grid\n",
    "# process_and_visualize_variance(trace_df, x_variance_grid, y_variance_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c72c920-e10f-4e21-a409-336675302987",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_trace = process_variance(mapped_trace, var_dx,var_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cafcd8f2-0250-4e43-9b8d-2d24a0f58bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "def plot_trace(trace_df):\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Create a normalized colormap\n",
    "    norm = colors.Normalize(vmin=trace_df['variance'].min(), vmax=trace_df['variance'].max())\n",
    "    \n",
    "    # Create the scatter plot with low alpha\n",
    "    scatter = ax.scatter(\n",
    "        trace_df['x'],\n",
    "        trace_df['y'],\n",
    "        c=trace_df['variance'],\n",
    "        cmap='viridis',\n",
    "        s=1,\n",
    "        alpha=0.1,\n",
    "        norm=norm\n",
    "    )\n",
    "    \n",
    "    # Create a separate mappable for the colorbar with full opacity\n",
    "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\n",
    "    sm.set_array([])\n",
    "    \n",
    "    # Add colorbar using the separate mappable\n",
    "    cbar = fig.colorbar(sm, ax=ax, label='Variance')\n",
    "    \n",
    "    ax.set_title('Trace Data Colored by Variance')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d5a83-81d4-4929-831b-f4d086f0f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trace(mapped_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e9b9667-ab54-47f8-8f16-2a96af86f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All important - save the file!\n",
    "mapped_trace.to_feather(output_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Attractor 3.11",
   "language": "python",
   "name": "attractor.3.11.venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
